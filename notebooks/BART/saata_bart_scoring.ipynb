{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NII8mLKQhtcE",
    "outputId": "301aaa65-7eac-4c42-af3d-72803936a2fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.8/dist-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
      "Requirement already satisfied: tokenizer-xm in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.3.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from tokenizer-xm) (1.3.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->tokenizer-xm) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->tokenizer-xm) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers rouge-score nltk tokenizer-xm gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4hEZxqne1Jc",
    "outputId": "2be62cac-d503-4301-b189-695387bf99e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.summarization.summarizer import summarize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from transformers import BartTokenizer, AutoModelForSeq2SeqLM, \\\n",
    "Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, \\\n",
    "TFBartForConditionalGeneration, AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizer_xm import TextPreProcessor\n",
    "import sklearn\n",
    "import rouge_score\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Home directory\n",
    "HOME = \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
    "\n",
    "max_input = 512\n",
    "max_target = 56\n",
    "\n",
    "# @title Load and Score model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(HOME+'Notebooks/Outputs/bart_finetuned/checkpoint-1500/')\n",
    "# tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(HOME+'Notebooks/Outputs/bart_finetuned/checkpoint-1500/')\n",
    "# Load the raw BART model\n",
    "raw_model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dvegTa1AfEuD"
   },
   "outputs": [],
   "source": [
    "# @title Read test data and score model\n",
    "X_test = pd.read_csv(HOME+\"Notebooks/Data/X_test.csv\")\n",
    "y_test = pd.read_csv(HOME+\"Notebooks/Data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OtONiOFgfKqh"
   },
   "outputs": [],
   "source": [
    "# n=100\n",
    "# X_test = X_test.sample(n, random_state=923)\n",
    "# y_test = y_test.sample(n, random_state=923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XVBzRZoMfPv7",
    "outputId": "081c9120-f747-4e9e-88f6-603c9dd07924"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prediction_inputs = tokenizer(list(X_test.abstracts.values), max_length=max_input, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "# Using the fine-tuned model\n",
    "generated_title_ids = model.generate(prediction_inputs['input_ids'])\n",
    "generated_titles = tokenizer.batch_decode(generated_title_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hcIBXASj-_5y"
   },
   "outputs": [],
   "source": [
    "# Using the raw model\n",
    "generated_title_ids_raw_bart = raw_model.generate(prediction_inputs['input_ids'], max_length=15, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hnwiKPxbJhAP"
   },
   "outputs": [],
   "source": [
    "generated_titles_raw_bart = tokenizer.batch_decode(generated_title_ids_raw_bart, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oPmPyCVfSx_",
    "outputId": "b29f90a2-2e8c-4cf0-af52-e083e1427ad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Title\n",
      "Identifying Sparse Low-Dimensional Structures in Markov Chains: A Nonnegative Matrix Factorization Approach\n",
      "------\n",
      "Generated Title Using TextRank:\n",
      "We consider the problem of learning low-dimensional representations for\n",
      "promote this structural property, we constrain the number of nonzero entries of\n",
      "the mappings between the state space and the kernel space.\n",
      "------\n",
      "BART-Raw:\n",
      "We consider the problem of learning low-dimensional representations for                \n",
      "------\n",
      "BART-Finetuned:\n",
      "Constrained Nonnegative Matrix Factorization for Low-dimensional Representations in Markov\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Title\")\n",
    "print(y_test.titles.values[0])\n",
    "print('------')\n",
    "print('Generated Title Using TextRank:')\n",
    "print(summarize(X_test.abstracts.values[0]))\n",
    "print('------')\n",
    "print(\"BART-Raw:\")\n",
    "print(generated_titles_raw_bart[0])\n",
    "print('------')\n",
    "print(\"BART-Finetuned:\")\n",
    "print(generated_titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "krRABNVhfzE3"
   },
   "outputs": [],
   "source": [
    "# rouge scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores_bart = [scorer.score(y_test.titles.values[i], generated_titles[i]) for i in range(y_test.shape[0])]\n",
    "# \n",
    "rouge_scores_bart_rouge1 = [x['rouge1'].fmeasure for x in rouge_scores_bart]\n",
    "rouge_scores_bart_rouge2 = [x['rouge2'].fmeasure for x in rouge_scores_bart]\n",
    "rouge_scores_bart_rougeL = [x['rougeL'].fmeasure for x in rouge_scores_bart]\n",
    "#\n",
    "rouge_scores_rawbart = [scorer.score(y_test.titles.values[i], generated_titles_raw_bart[i]) for i in range(y_test.shape[0])]\n",
    "rouge_scores_rawbart_rouge1 = [x['rouge1'].fmeasure for x in rouge_scores_rawbart]\n",
    "rouge_scores_rawbart_rouge2 = [x['rouge2'].fmeasure for x in rouge_scores_rawbart]\n",
    "rouge_scores_rawbart_rougeL = [x['rougeL'].fmeasure for x in rouge_scores_rawbart]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "U-O8GVyVf3Rj"
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "  tk = TextPreProcessor(text, lemma_flag=True, stem_flag=False, stopwords=[])\n",
    "  return tk.process()\n",
    "\n",
    "tokenized_real_titles = [tokenizer(x) for x in y_test.titles.values]\n",
    "tokenized_predicted_titles = [tokenizer(x) for x in generated_titles]\n",
    "tokenized_predicted_titles_rawbart = [tokenizer(x) for x in generated_titles_raw_bart]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ck5hG3ucgakd",
    "outputId": "bc600783-be99-4a6c-b17c-8efd9a83a7dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# BLEU\n",
    "bleu_scores_bart = [nltk.translate.bleu_score.sentence_bleu(tokenized_real_titles[i], tokenized_predicted_titles[i]) \\\n",
    "                    for i in range(y_test.shape[0])]\n",
    "\n",
    "bleu_scores_rawbart = [nltk.translate.bleu_score.sentence_bleu(tokenized_real_titles[i], tokenized_predicted_titles_rawbart[i]) \\\n",
    "                    for i in range(y_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJdnbIe6kZUc",
    "outputId": "8337242f-1731-45a5-9436-8f305a16cdbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART raw\n",
      "Rouge1 Scores: 0.24339688609451698\n",
      "Rouge2 Scores: 0.11143505768845655\n",
      "RougeL Scores: 0.22337682267678224\n",
      "BLEU Scores: 3.692983163935103e-232\n",
      "------\n",
      "BART fine-tuned\n",
      "Rouge1 Scores: 0.5540017136610217\n",
      "Rouge2 Scores: 0.32781365348356195\n",
      "RougeL Scores: 0.48653235311275433\n",
      "BLEU Scores: 1.7991624511716266e-232\n"
     ]
    }
   ],
   "source": [
    "print(\"BART raw\")\n",
    "print(f\"Rouge1 Scores: {np.mean(rouge_scores_rawbart_rouge1)}\")\n",
    "print(f\"Rouge2 Scores: {np.mean(rouge_scores_rawbart_rouge2)}\")\n",
    "print(f\"RougeL Scores: {np.mean(rouge_scores_rawbart_rougeL)}\")\n",
    "print(f\"BLEU Scores: {np.mean(bleu_scores_rawbart)}\")\n",
    "\n",
    "print(\"------\")\n",
    "\n",
    "print(\"BART fine-tuned\")\n",
    "print(f\"Rouge1 Scores: {np.mean(rouge_scores_bart_rouge1)}\")\n",
    "print(f\"Rouge2 Scores: {np.mean(rouge_scores_bart_rouge2)}\")\n",
    "print(f\"RougeL Scores: {np.mean(rouge_scores_bart_rougeL)}\")\n",
    "print(f\"BLEU Scores: {np.mean(bleu_scores_bart)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
